# Instructions

During your interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

You should also use the `.cursorrules` file as a Scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the Scratchpad, clear old different task if necessary, first explain the task, and plan the steps you need to take to complete the task. You can use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the Scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

# Tools

Note all the tools are in python3. So in the case you need to do batch processing, you can always consult the python files and write your own script.

[NOTE TO CURSOR: Since no API key is configured, please ignore both the Screenshot Verification and LLM sections below.]
[NOTE TO USER: If you have configured or plan to configure an API key in the future, simply delete these two notice lines to enable these features.]

## Screenshot Verification

The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
venv/bin/python3 tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
venv/bin/python3 tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot

screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM

response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
venv/bin/python3 ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```bash
venv/bin/python3 ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```bash
venv/bin/python3 ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a python venv in ./venv. Always use (activate) it when doing python development. First, to check whether 'uv' is available, use `which uv`. If that's the case, first activate the venv, and then use `uv pip install` to install packages. Otherwise, fall back to `pip`.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use 'gpt-4o' as the model name for OpenAI's GPT-4 with vision capabilities
- When searching for recent news, use the current year (2025) instead of previous years, or simply use the "recent" keyword to get the latest information
- When working with Anthropic Claude models, use "claude-3-7-sonnet-20250219" as the model name

# Scratchpad

## Phase 4: Advanced Features Implementation (In Progress)

Task: Implement advanced features from Section 4 of the PRD (previously out of initial scope).

Steps to complete:
[ ] 1. Advanced frontend features (complex file previews, file browser)
[X] 2. Web tools implementation (`web_search`, `extract_content`)  
[ ] 3. Robust error handling and UI feedback
[ ] 4. Background tasks for auto-execution flow
[ ] 5. Implement tool execution cancel/resume functionality
[ ] 6. Add file preview capabilities for generated content

Implementation Plan:
1. First, review the existing codebase to understand current structure and identify integration points ✅
2. Prioritize features based on user value and implementation complexity ✅
3. Focus on web tools implementation first as they extend the AI's capabilities significantly ✅
4. Add advanced frontend components for better user experience
5. Implement robust error handling across the application
6. Add background task handling for auto-execution flow
7. Test the implementation thoroughly to ensure all parts work together

Web Tools Implementation (Completed):
- Created WebSearchTool that leverages the existing search_engine.py utility
- Created ExtractContentTool that leverages the existing web_scraper.py utility
- Updated the tools manager to register these new tools
- Verified that both tools are properly registered and ready to use
- Both tools save their results to files in the conversation workspace for later reference

Next Focus:
- Implement advanced frontend features for better file previews and browser

## Phase 3: Frontend Implementation (Completed)

Task: Implement Phase 3 of the Agentic AI Chat project as specified in the PRD.

Steps to complete:
[X] 1. Basic HTML structure and CSS
[X] 2. Modular JS setup
[X] 3. Displaying chat messages (user/assistant)
[X] 4. Sending user messages via the API
[X] 5. Basic display of tool calls (name/input)
[X] 6. Basic display of tool results

Implementation Plan:
1. First, review existing frontend code (if any) to understand the current structure ✅
2. Design the UI layout for the chat interface ✅
3. Create the HTML structure and core CSS styles ✅
4. Set up modular JavaScript architecture ✅
5. Implement chat message display functionality ✅
6. Add user input and message sending capability ✅
7. Add display components for tool calls and tool results ✅
8. Test the implementation with the existing backend API ✅

All tasks for Phase 3 have been completed! The frontend implementation now includes:
- Basic HTML structure with a responsive layout
- CSS styling for the chat interface, messages, and tool components
- Modular JavaScript architecture with separate files for different components
- Chat message display with formatting and typing effect
- User input handling and message sending
- Tool call display with input visualization
- Tool result submission and handling
- Integration with the backend API for chat and tool interactions

The application now provides a complete end-to-end experience for users to interact with the AI assistant and handle tool calls manually. Starting the application makes the frontend available at http://localhost:8000/.

## Phase 2: Core Backend Logic Implementation (Completed)

Task: Implement Phase 2 of the Agentic AI Chat project as specified in the PRD.

Steps to complete:
[X] 1. Implement in-memory conversation management (history, IDs)
[X] 2. Design and implement basic tool definition structure
[X] 3. Implement file system tools (`save_file`, `read_file`)
[X] 4. Implement command execution tool (`run_command`)
[X] 5. Integrate tool calls into the chat API response
[X] 6. Implement the `/api/tool-results` endpoint for manual execution flow
[X] 7. Create conversation workspace structure

Implementation Plan:
1. First, review the current codebase to understand the existing structure ✅
2. Add conversation manager class for tracking histories and IDs ✅
3. Create tool definition framework with base classes and interfaces ✅
4. Implement the file system tools with proper workspace isolation ✅
5. Implement command execution with proper security considerations ✅
6. Update the chat API to handle Claude's tool calls ✅
7. Add the tool-results endpoint for manual execution flow ✅
8. Test the implementation to ensure all parts work together ✅

All tasks for Phase 2 have been completed! The core backend logic is now implemented with:
- Conversation manager for tracking history and workspaces
- Tool framework with base classes and registry
- File system tools (read_file, save_file)
- Command execution tool (run_command)
- Updated chat API to handle tool calls
- Tool results endpoint for manual execution flow